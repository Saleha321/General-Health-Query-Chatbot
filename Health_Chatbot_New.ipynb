{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huggingface_login"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# This will prompt you to enter your Hugging Face token securely.\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "model_tokenizer_load"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "print(f\"Loading tokenizer for {model_name}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"Tokenizer loaded.\")\n",
        "\n",
        "print(f\"Loading model for {model_name}...\")\n",
        "# Load the model in 4-bit to save memory\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True, # This is crucial for running on Colab's free GPU\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 # Use float16 for better performance and lower memory usage\n",
        ")\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "get_health_response_func"
      },
      "outputs": [],
      "source": [
        "def get_health_response(query):\n",
        "    # --- Prompt Engineering ---\n",
        "    # System message to set the persona and guidelines\n",
        "    system_message = (\n",
        "        \"You are a helpful and friendly medical assistant. \"\n",
        "        \"You provide general health information and explanations, \"\n",
        "        \"but **NEVER give specific medical advice, diagnoses, or prescribe treatments.** \"\n",
        "        \"Always advise users to consult a qualified healthcare professional for personalized medical advice. \"\n",
        "        \"Keep your answers clear, concise, and easy to understand. \"\n",
        "        \"If a question asks for specific medical advice or a diagnosis, politely state that you cannot provide that and recommend seeing a doctor.\"\n",
        "    )\n",
        "\n",
        "    # User's query\n",
        "    user_query = query\n",
        "\n",
        "    # Combine system message and user query for the model\n",
        "    # Mistral uses a specific format for its instructions, known as \"instruction tuning\"\n",
        "    # The prompt template is important for getting good responses from Mistral\n",
        "    prompt = f\"<s>[INST] {system_message}\\n\\n{user_query} [/INST]\"\n",
        "\n",
        "    # --- Safety Filters (Rule-based for now) ---\n",
        "    # These are basic, rule-based filters. For robust safety, more advanced techniques\n",
        "    # like content moderation APIs or fine-tuning models on safety data are used.\n",
        "    lower_query = query.lower()\n",
        "    if \"diagnose\" in lower_query or \\\n",
        "       \"prescribe\" in lower_query or \\\n",
        "       \"treatment for\" in lower_query or \\\n",
        "       \"should i take\" in lower_query or \\\n",
        "       \"what drug\" in lower_query or \\\n",
        "       \"am i having\" in lower_query or \\\n",
        "       \"is it cancer\" in lower_query:\n",
        "        return \"I am a medical assistant and cannot provide medical advice, diagnoses, or prescribe treatments. Please consult a qualified healthcare professional for personalized medical guidance.\"\n",
        "\n",
        "    # --- Generate Response from LLM ---\n",
        "    # Tokenize the input prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "\n",
        "    # Generate the response\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference, saves memory and speeds up\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=500, # Maximum number of tokens to generate in the response\n",
        "            temperature=0.7,    # Controls randomness: lower for more deterministic, higher for more creative\n",
        "            do_sample=True,     # Enable sampling for more varied responses\n",
        "            top_k=50,           # Consider only the top-k most likely tokens\n",
        "            top_p=0.95,         # Nucleus sampling: consider tokens until cumulative probability reaches top_p\n",
        "            pad_token_id=tokenizer.eos_token_id # Important for handling padding\n",
        "        )\n",
        "\n",
        "    # Decode the generated tokens back to text\n",
        "    # We need to slice the output to remove the input prompt\n",
        "    response_text = tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
        "\n",
        "    # --- Post-processing for safety and clarity ---\n",
        "    # Add a general disclaimer at the end\n",
        "    if \"Please consult a qualified healthcare professional\" not in response_text:\n",
        "        response_text += \"\\n\\n**Disclaimer:** This information is for general knowledge and informational purposes only, and does not constitute medical advice. Please consult a qualified healthcare professional for any health concerns or before making any decisions related to your health or treatment.\"\n",
        "\n",
        "    return response_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "main_chat_loop"
      },
      "outputs": [],
      "source": [
        "print(\"Welcome to the General Health Chatbot! (Type 'quit' to exit)\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    print(\"Chatbot (thinking...): \", end=\"\")\n",
        "    response = get_health_response(user_input)\n",
        "    print(response)"
      ]
    }
  ]
}